{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PA7pp8KW8sy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.core import ServiceContext\n",
        "from llama_index.legacy.embeddings.langchain import LangchainEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "#default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|> {query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "hO2F8bN5XH3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\":0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\":True}\n",
        "\n",
        ")\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "embed_model = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "XjLvRWiuXMJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "Med3e6EnXNKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, to load the persisted index and use it:\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# Load the storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"/content/storage\")\n",
        "\n",
        "# Load the index from the storage\n",
        "loaded_index = load_index_from_storage(storage_context, service_context=service_context)\n",
        "\n",
        "# Use the loaded query engine to answer your query\n",
        "response = loaded_query_engine.query(\"What are the OTC medication for alopecia areata?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "XGR11-aFXgUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}